{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d843cb2e",
   "metadata": {},
   "source": [
    "# Online Retail Market Segmentation\n",
    "### Objectives:\n",
    "Segment customers into distinct groups based on their purchasing behavior to enable personalized marketing strategies and uncover deeper insights into the firm's customer base.\n",
    "### Data:\n",
    "This dataset contains transactional records from a UK-based, non-store online retailer, covering all transactions between December 1, 2010, and December 9, 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4125cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from rapidfuzz import fuzz, process\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from kneed import KneeLocator\n",
    "import squarify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbd6f8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 541909 entries, 0 to 541908\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   InvoiceNo    541909 non-null  object        \n",
      " 1   StockCode    541909 non-null  object        \n",
      " 2   Description  540455 non-null  object        \n",
      " 3   Quantity     541909 non-null  int64         \n",
      " 4   InvoiceDate  541909 non-null  datetime64[ns]\n",
      " 5   UnitPrice    541909 non-null  float64       \n",
      " 6   CustomerID   406829 non-null  float64       \n",
      " 7   Country      541909 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(2), int64(1), object(4)\n",
      "memory usage: 33.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('Online_Retail_2.xlsx')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd364bd1",
   "metadata": {},
   "source": [
    "## Characterizing the data \n",
    "There are a total of 8 columns and 541,909 entries in the initial dataset. Four columns have an object data type, one is int64, two are float64, and one is datetime64."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0976c6",
   "metadata": {},
   "source": [
    "### Data Type Conversion\n",
    "*CustomerID* should be consider as string(category) instead of float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bbabe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data type of CustomerID to string\n",
    "df['CustomerID'] = df['CustomerID'].astype('Int64').astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc01335",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "Based on the data summary, *CustomerID* serves as the primary identifier for this analysis, as customers represent the central element of the study. Since this field contains missing values, all records lacking a *CustomerID* were excluded. Following this step, the dataset was reduced from its original size by 135,080 records, leaving 406,829 valid entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80058a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total null values of CustomerID: 0\n",
      "Incorrect customer id: 135080\n",
      "['<NA>']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 406829 entries, 0 to 541908\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   InvoiceNo    406829 non-null  object        \n",
      " 1   StockCode    406829 non-null  object        \n",
      " 2   Description  406829 non-null  object        \n",
      " 3   Quantity     406829 non-null  int64         \n",
      " 4   InvoiceDate  406829 non-null  datetime64[ns]\n",
      " 5   UnitPrice    406829 non-null  float64       \n",
      " 6   CustomerID   406829 non-null  object        \n",
      " 7   Country      406829 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(5)\n",
      "memory usage: 27.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# CustomerID\n",
    "# Find the total null values of CustomerID\n",
    "print(f'Total null values of CustomerID: {df['CustomerID'].isna().sum()}')\n",
    "\n",
    "# # Check if CustomerID is digits formed string\n",
    "ID_not_digit = df[~df['CustomerID'].str.isdigit()]\n",
    "print(f'Incorrect customer id: {len(ID_not_digit)}') \n",
    "print(ID_not_digit['CustomerID'].unique())          \n",
    "df['CustomerID'].replace('<NA>', np.nan, inplace=True) \n",
    "\n",
    "# # Drop the null values\n",
    "df = df.dropna(subset=['CustomerID'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa004e7",
   "metadata": {},
   "source": [
    "## Treating Unrealistic Senarios\n",
    "\n",
    "1. Quantity equal to zero or negaive\n",
    "    - Total of 10,624 entires. After cleaning, df contains total of 531,285 entires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6de1793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of having 0 or negative quantity: 8905\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 397924 entries, 0 to 541908\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   InvoiceNo    397924 non-null  object        \n",
      " 1   StockCode    397924 non-null  object        \n",
      " 2   Description  397924 non-null  object        \n",
      " 3   Quantity     397924 non-null  int64         \n",
      " 4   InvoiceDate  397924 non-null  datetime64[ns]\n",
      " 5   UnitPrice    397924 non-null  float64       \n",
      " 6   CustomerID   397924 non-null  object        \n",
      " 7   Country      397924 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(5)\n",
      "memory usage: 27.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# check if there are order quantity equal to zero or nagative and drop it\n",
    "num_0_neg_quantity = df[(df['Quantity'] == 0) | (df['Quantity'] < 0)]\n",
    "print(f'Number of having 0 or negative quantity: {len(num_0_neg_quantity)}')\n",
    "\n",
    "df.drop(num_0_neg_quantity.index, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab8cccc",
   "metadata": {},
   "source": [
    "### Treating inconsistency of Data representation\n",
    "1. Check if Description has mispelling items.\n",
    "2. Check if country names are correctly represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf79896e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4110369820.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 15\u001b[0;36m\u001b[0m\n\u001b[0;31m    =\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Check the number of Description before cleaning\n",
    "descriptions = df['Description'].unique()\n",
    "print(len(descriptions))\n",
    "\n",
    "threshold = 90\n",
    "\n",
    "# Find near duplicates\n",
    "near_duplicates = []\n",
    "\n",
    "for i, desc1 in enumerate(descriptions):\n",
    "    for desc2 in descriptions[i+1:]:\n",
    "        score = fuzz.ratio(desc1, desc2)\n",
    "        if score >= threshold:\n",
    "            near_duplicates.append((desc1, desc2, score))\n",
    "=\n",
    "for d1, d2, score in near_duplicates:\n",
    "    if score > 92:\n",
    "        print(f\"{d1} ~ {d2} (similarity: {score}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71644178",
   "metadata": {},
   "source": [
    "We realized that there are descriptions indicating the same items but identified as different objects. For example: \n",
    "\n",
    "\" SET 2 TEA TOWELS I LOVE LONDON \": \"SET 2 TEA TOWELS I LOVE LONDON\",\n",
    "\n",
    "\"GIN + TONIC DIET METAL SIGN\": \"GIN AND TONIC DIET METAL SIGN\",\n",
    "\n",
    "\"FAIRY TALE COTTAGE NIGHTLIGHT\": \"FAIRY TALE COTTAGE NIGHT LIGHT\",\n",
    "\n",
    "\"BATHROOM METAL SIGN \": \"BATHROOM METAL SIGN\",\n",
    "\n",
    "\"LUNCH BAG SUKI  DESIGN \": \"LUNCH BAG SUKI DESIGN\"\n",
    "...\n",
    "\n",
    "\n",
    "Since this market segmentation is focused on customers, it doesn’t require a fully cleaned Description column. However, it is important to recognize that the Description column still contains some inconsistencies and duplicates. Let’s minimize incorrect descriptions by trimming them properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b63b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triming the Description to minimalize the overlapping.\n",
    "df['Description'] = df['Description'].str.strip()\n",
    "df['Description'] = df['Description'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Check how many items got cleaned. \n",
    "df['Description'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad23ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if country names are correctly represented.\n",
    "df['Country'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af91b7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Country'] = df['Country'].replace({\n",
    "    'EIRE': 'Eire',\n",
    "    'USA': 'United States',\n",
    "    'RSA': 'South Africa'\n",
    "})\n",
    "df['Country'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeac65e",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c2cf68",
   "metadata": {},
   "source": [
    "### Explore Unique Values\n",
    "Understand the relationship and dataset:\n",
    "1. The number of *Description*: 3858, and the number of *StockCode*: 3665\n",
    "--> This means that there are *Descriptions* that are not cleaned, even though *StockCode* and *Description* usually have a one-to-one relationship.\n",
    "2. There are total of 4,339 customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba6c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = df.nunique().reset_index()\n",
    "unique_counts.columns = ['Column', 'Unique_Values']\n",
    "unique_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369d4c36",
   "metadata": {},
   "source": [
    "## Explore Numerical Data\n",
    "From the sumamry statistic of *Unit Price* and *Quantity*, we identified the following:\n",
    "\n",
    "Unit Price:\n",
    "1. There are high-value outliers, as the mean (3.12) is greater than the median (1.95), indicating a positive (right) skew.\n",
    "2. The minimum is 0, which means there are products listed at no cost.\n",
    "3. The IQR is: Q3 − Q1 = 3.75 − 1.25 = 2.5.\n",
    "\n",
    "Quantity:\n",
    "1. The mean (13.02) is greater than the median (6), indicating a positive (right) skew. The standard deviation (180.42) is large compared to the mean (13), which confirms the existence of outliers.\n",
    "2. The maximum is 80,995, which is an outlier.\n",
    "3. The IQR is: Q3 − Q1 = 12 − 2 = 10.\n",
    "\n",
    "Conclusion: Positive(right) skews with high value outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ffb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expore the unit price and Quantity\n",
    "df[['UnitPrice', 'Quantity']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8718336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many rows with free products\n",
    "print(f'Unit price is equal to zero: {(df['UnitPrice'] == 0).sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a292a18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot for Unit Price\n",
    "plt.figure(figsize=(2, 4)) \n",
    "df['UnitPrice'].plot(kind='box')\n",
    "plt.title('Unit Price Box Plot')\n",
    "plt.show()\n",
    "\n",
    "# Box plot for Quantity\n",
    "plt.figure(figsize=(2, 4))\n",
    "df['Quantity'].plot(kind='box')\n",
    "plt.title('Quantity Box Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f349005",
   "metadata": {},
   "source": [
    "From the box plots above, we can see that most unit prices are below $1,000, with some data between $1,000 and $2,000, and 4 observations exceeding $3,000.\n",
    "\n",
    "For quantity, most data are concentrated under 10,000 units. There are three orders that are extremely large, exceeding 10,000 units.\n",
    "\n",
    "Both Unit Price and Quantity are positively (right) skewed; however, Quantity shows a more pronounced right skew. Both variables have extreme outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b10570",
   "metadata": {},
   "source": [
    "### Create useful data\n",
    "Monetary Value = Quantity * UnitPrice\n",
    "\n",
    "Monetary value can be consider as the money that expect to receive from selling something. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6661fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns monetary value. \n",
    "df['MonetaryValue'] = df['Quantity'] * df['UnitPrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15f8027",
   "metadata": {},
   "source": [
    "## Explore Data\n",
    "1. Top 10 selling products and their sales share: *Description × Monetary Value*.\n",
    "\n",
    "2. Top 10 products generating the most quantity (most popular): *Description × Quantity*.\n",
    "\n",
    "3. Top 10 customers with the highest spending.\n",
    "\n",
    "4. Number of customers who purchased only once: *InvoiceDate × CustomerID*\n",
    "\n",
    "    → There are a total of 1,556 customers who purchased only once, which is 35.86% of all customers.\n",
    "\n",
    "5. Top 10 customers with the longest time span of purchases: *InvoiceDate × CustomerID*.\n",
    "\n",
    "6. Detect seasonality or demand spikes: *InvoiceDate × InvoiceNo*\n",
    "\n",
    "    → We recognize that the dataset spans only one year, making it difficult to detect clear seasonality. However, from the available data, early to mid-November shows the most transactions.\n",
    "\n",
    "7. Total sales per month: *InvoiceDate × Monetary Value*\n",
    "\n",
    "    → Calculating total sales per month reinforces that November is the key month, with the most transactions and highest sales. Total sales started increasing from September and peaked in November.\n",
    "\n",
    "8. Find peak shopping hours: *InvoiceDate × InvoiceNo*\n",
    "\n",
    "    → Peak hours are around noon. As the time moves away from 12 PM, transaction counts decrease.\n",
    "\n",
    "9. Sales by region: *Monetary Value × Country*\n",
    "\n",
    "    → Most orders are placed by customers from the United Kingdom.\n",
    "\n",
    "10. Customers by region: *CustomerID × Country*\n",
    "\n",
    "    → About 92.6% of customers are from the United Kingdom, followed by Germany, France, and Spain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e156223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top 10 selling products and their sales share\n",
    "# Create a table that has products' total sales\n",
    "top_selling = df.groupby('Description')['MonetaryValue'].sum().sort_values(ascending=False).head(10)\n",
    "top_selling_df = top_selling.reset_index()\n",
    "top_selling_df.rename(columns={'MonetaryValue': 'TotalSales'}, inplace=True)\n",
    "\n",
    "total_sales = df['MonetaryValue'].sum()\n",
    "\n",
    "# Compute Sales Share: products' total sales/all products total sales\n",
    "top_selling_df['SalesShare'] = top_selling_df['TotalSales'] / total_sales * 100\n",
    "top_selling_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe9036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the product generate the most quantity\n",
    "top_quantity = df.groupby('Description')['Quantity'].sum().sort_values(ascending=False).head(10)\n",
    "top_quantity_df = top_quantity.reset_index()\n",
    "top_quantity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef72da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 Customers with most spending\n",
    "top_selling_customer = df.groupby('CustomerID')['MonetaryValue'].sum().sort_values(ascending=False).head(10)\n",
    "top_selling_customer_df = top_selling_customer.reset_index()\n",
    "top_selling_customer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4896285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the time span of the dataset\n",
    "start_date = df['InvoiceDate'].min()\n",
    "end_date = df['InvoiceDate'].max()\n",
    "time_span = end_date - start_date\n",
    "\n",
    "print(f\"Dataset covers from {start_date} to {end_date} ({time_span.days} days).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6654d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer that only purchased once = time_span == 0\n",
    "customer_span = df.groupby('CustomerID')['InvoiceDate'].agg(['min', 'max'])\n",
    "customer_span['time_span'] = customer_span['max'] - customer_span['min']\n",
    "\n",
    "zero_span_customers = customer_span[customer_span['time_span'].dt.days == 0]\n",
    "print(f'Number of customer that only purchased once: {len(zero_span_customers)}')\n",
    "print(f'Percentage of customer that only purchased once: {(len(zero_span_customers)/df['CustomerID'].nunique())*100}')\n",
    "\n",
    "one_time_count = len(zero_span_customers)\n",
    "multi_time_count = df['CustomerID'].nunique() - one_time_count\n",
    "\n",
    "labels = ['One-Time Customers', 'Repeat Customers']\n",
    "sizes = [one_time_count, multi_time_count]\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(\n",
    "    sizes,\n",
    "    labels=labels,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    wedgeprops={'edgecolor': 'white'}\n",
    ")\n",
    "plt.title('Customer Purchase Behavior')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f03bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 customers with longest time spam\n",
    "top10_timespam = customer_span['time_span'].sort_values(ascending=False).head(10)\n",
    "top10_timespam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864032a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect seasonality or demand spikes\n",
    "transactions_per_day = df.groupby(df['InvoiceDate'].dt.date).size()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(transactions_per_day.index, transactions_per_day.values)\n",
    "plt.title('Number of Transactions per Day')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ce016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Sales per Month\n",
    "df['YearMonth'] = df['InvoiceDate'].dt.to_period('M')\n",
    "\n",
    "monthly_sales = df.groupby('YearMonth')['InvoiceNo'].sum()\n",
    "best_month = monthly_sales.idxmax()\n",
    "best_value = monthly_sales.max()\n",
    "\n",
    "print(f\"The best month was {best_month} with total sales of {best_value:,.2f}\")\n",
    "\n",
    "monthly_sales.plot(kind='bar', figsize=(10,5))\n",
    "plt.title('Total Sales per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4456b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find peak shopping hours\n",
    "df['Hour'] = df['InvoiceDate'].dt.hour\n",
    "\n",
    "hourly_sales = df.groupby('Hour')['InvoiceNo'].nunique()\n",
    "\n",
    "# Find peak hour\n",
    "best_hour = hourly_sales.idxmax()\n",
    "best_value = hourly_sales.max()\n",
    "\n",
    "print(f\"The peak shopping hour is {best_hour}:00 with {best_value:,} invoices.\")\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "hourly_sales.plot(kind='bar')\n",
    "plt.title('Total Transactions by Hour of Day')\n",
    "plt.xlabel('Hour of Day (0–23)')\n",
    "plt.ylabel('Number of Invoices')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e429467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales by region\n",
    "region_sales = df.groupby('Country')['MonetaryValue'].sum().sort_values(ascending=False).head(5)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "region_sales.head(10).plot(kind='bar')\n",
    "plt.title('Total Sales by Region')\n",
    "plt.xlabel('Region / Country')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40412dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer by region\n",
    "region_customers = df.groupby('Country')['CustomerID'].nunique().sort_values(ascending=False)\n",
    "top_regions = region_customers\n",
    "regcolors = plt.cm.tab20.colors\n",
    "\n",
    "plt.pie(\n",
    "    region_customers.head(10),\n",
    "    labels=None,\n",
    "    autopct='%1.0f%%',\n",
    "    startangle=90,\n",
    "    counterclock=False,\n",
    "    colors=regcolors\n",
    ")\n",
    "\n",
    "plt.legend(region_customers.head(10).index, title=\"Region\", bbox_to_anchor=(1, 0.9))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c7ff3e",
   "metadata": {},
   "source": [
    "## RFM Analysis\n",
    "RFM stands for Recency, Frequency, and Monetary Value — a marketing analysis framework used for customer segmentation and behavioral targeting. It helps businesses identify their most valuable customers based on purchasing behavior, enabling data-driven decisions to improve strategy and performance.\n",
    "\n",
    "Recency: How recently a customer made their last purchase.\n",
    "\n",
    "Frequency: How often a customer makes purchases.\n",
    "\n",
    "Monetary Value: How much a customer spends.\n",
    "\n",
    "The main objective of RFM analysis is to identify high-value customers and understand purchasing patterns. By leveraging these insights, businesses can optimize marketing strategies, improve product positioning, and increase revenue through better customer engagement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8266e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RFM Table\n",
    "refrence_date = df['InvoiceDate'].max() + pd.Timedelta(days = 1)\n",
    "\n",
    "rfm = df.groupby('CustomerID').agg({\n",
    "    'InvoiceDate': lambda x: (refrence_date - x.max()).days,  # Recency\n",
    "    'InvoiceNo': 'nunique',                                   # Frequency\n",
    "    'MonetaryValue': 'sum'                                    # Monetary\n",
    "})\n",
    "\n",
    "rfm.rename(columns={\n",
    "    'InvoiceDate': 'Recency',\n",
    "    'InvoiceNo': 'Frequency',\n",
    "    'Sales': 'Monetary'\n",
    "}, inplace=True)\n",
    "\n",
    "rfm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155d8f6d",
   "metadata": {},
   "source": [
    "### Check RFM data and its distribution\n",
    "An initial check of the RFM data is crucial, as it can significantly affect the clustering results. It’s important to understand that RFM variables are typically highly skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff3041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of Recency, Frequency, Monetary Value\n",
    "# Plot Distribution of Recency\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(rfm['Recency'], bins=30, color='skyblue', edgecolor=None, stat='density')\n",
    "sns.kdeplot(rfm['Recency'], color='red', linewidth=2)\n",
    "plt.title('Distribution of Recency with Density Curve')\n",
    "plt.xlabel('Recency')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()\n",
    "\n",
    "# Plot Distribution of Frequency\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(rfm['Frequency'], bins=30, color='skyblue', edgecolor=None, stat='density')\n",
    "sns.kdeplot(rfm['Frequency'], color='red', linewidth=2)\n",
    "plt.title('Distribution of Frequency with Density Curve')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()\n",
    "\n",
    "# Plot Distribution of Monetary \n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(rfm['MonetaryValue'], bins=30, color='skyblue', edgecolor=None, stat='density')\n",
    "sns.kdeplot(rfm['MonetaryValue'], color='red', linewidth=2)\n",
    "plt.title('Distribution of Monetary Value with Density Curve')\n",
    "plt.xlabel('Monetary Value')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2ef4c4",
   "metadata": {},
   "source": [
    "## RFM analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f96d623",
   "metadata": {},
   "source": [
    "### Label RFM Score \n",
    "Labeling RFM scores might not be crucial for K-means clustering; however, it still provides better insight into customer placement and makes it easier for marketing or CRM teams to understand the segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c43210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label RFM Score\n",
    "# Recency: lower value = better, so invert labels\n",
    "rfm['R_Score'] = pd.qcut(rfm['Recency'], 4, labels=[4,3,2,1])\n",
    "\n",
    "# Frequency: higher value = better\n",
    "rfm['F_Score'] = pd.qcut(rfm['Frequency'].rank(method='first'), 4, labels=[1,2,3,4])\n",
    "\n",
    "# Monetary: higher value = better\n",
    "rfm['M_Score'] = pd.qcut(rfm['MonetaryValue'], 4, labels=[1,2,3,4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03a9632",
   "metadata": {},
   "source": [
    "## K-Mean Clustering\n",
    "\n",
    "Understanding the K-Means:\n",
    "\n",
    "The objectives of K-Means clustering is to group similar data point, in our senarios, which is grouping customer into different segmens. It aim to minimize the inertia which is the distance between each data points to the centroids, which is what within cluster distance (using the Euclidean distance). tight kuit clusers showing a higher cohesiveness of the group. Then K-Means try to maximizing between clusters, which means each cluster can be more distinct. \n",
    "\n",
    "key concepts:\n",
    "\n",
    "Inertia -- how far the points within a cluster are. \n",
    "--> lower the inertia is, better the K-Mean clustering. \n",
    "\n",
    "Dunn Index -- how far between clusters.\n",
    "--> higher the Dunn Index, Better the K-Mean clustering is. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9d2ae4",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Data Preprocessing is crucial for K-Mean Clustering as the raw data usually highly skewed and containing outliers. K-Means clustering works by minimizing the sum of squared distances between data points and cluster centroids. As K-Means uses Euclidean distance, it is sensitive to large values, which the outliers. \n",
    "\n",
    "We are going to applied Log Transformation and Z-score normalization to standardize our datasets. \n",
    "\n",
    "Log Transformation:\n",
    "- It reduce skewness and compress large values while keeping the order of magnitude. Reducing the impact of extreme values and helps clusters form based on relative differences, improving the quality of the clustering. \n",
    "\n",
    "Z-score Normalization:\n",
    "- It scale features to have mean = 0 and standard deviation =1, so all features contribute equally to clustering. The formula is z=(x-mu)/sigma. Z-score normalization ensures all features are on teh same scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea331fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nomalize the data using log Transformation and z-score normalization\n",
    "rfm_copy = rfm.copy()\n",
    "\n",
    "rfm_copy['recency_log'] = np.log(rfm_copy['Recency'] + 1)\n",
    "rfm_copy['frequency_log'] = np.log(rfm_copy['Frequency'] + 1)\n",
    "rfm_copy['monetary_log'] = np.log(rfm_copy['MonetaryValue'] + 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(rfm_copy[['recency_log', 'frequency_log', 'monetary_log']])\n",
    "\n",
    "rfm_norm = pd.DataFrame(X_scaled, columns=['Recency_Norm','Frequency_Norm','Monetary_Norm'])\n",
    "rfm_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b00b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the Correlation of RFM Features\n",
    "corr_matrix = rfm_norm.corr()\n",
    "print(corr_matrix)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=True, \n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1, vmax=1\n",
    ")\n",
    "plt.title('Correlation between Normalized RFM Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ae2c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the RFM features\n",
    "sns.pairplot(\n",
    "    rfm_norm,\n",
    "    kind='reg',\n",
    "    diag_kind='kde',\n",
    "    plot_kws={'scatter_kws': {'alpha':0.5, 's':20}, 'line_kws': {'color':'red'}}\n",
    ")\n",
    "plt.suptitle('RFM Regression Pair Plots', y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41247815",
   "metadata": {},
   "source": [
    "From the heatmaps and pairplot we generated, we observe a strong positive relationship between Monetary Value and Frequency (correlation = 0.81), indicating that customers who purchase more frequently tend to contribute higher sales. \n",
    "\n",
    "Additionally, there is a moderate negative relationship between Recency and Frequency (correlation = -0.57), as well as between Recency and Monetary Value (correlation = -0.48). This suggests that the longer the time since a customer’s last purchase, the less frequently they make purchases, and the lower the total amount they spend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0d52f7",
   "metadata": {},
   "source": [
    "## Elbow Method\n",
    "Elbow method is widely used technique in clustering (especially K-Means) to help determine the optimal number of clusters (k). \n",
    "\n",
    "It run K-Means for a range of K values, and for each K, it calculate the Within_Cluster Sum of Squares (WCSS), aka inertia. \n",
    "\n",
    "Usually as k increases, WCSS always decreases as more clusters means smaller distances to centroids. \n",
    "\n",
    "Optimal number of cluster is the one that balancing cluster compactness and simplicity. Where the WCSS curve starts to flatten. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c19c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SSE (inertia) for different K\n",
    "sse = []  # Sum of squared errors\n",
    "kmeans_kwargs = {\n",
    "    \"init\": \"k-means++\", \n",
    "    \"n_init\": 10,\n",
    "    \"max_iter\": 300,\n",
    "    \"random_state\": 127,\n",
    "}\n",
    "K_range = range(1, 21) \n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(X_scaled)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "# Apply KneeLocator to find the optimal k\n",
    "kl = KneeLocator(K_range, sse, curve=\"convex\", direction=\"decreasing\")\n",
    "\n",
    "print(f\"Optimal number of clusters (Elbow Method): {kl.elbow}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(K_range, sse, marker='o')\n",
    "plt.xticks(K_range)\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Sum of Squared Errors (SSE)\")\n",
    "plt.title(\"Elbow Method for Optimal K\")\n",
    "plt.axvline(x=kl.elbow, color='red', linestyle='--', label=f'Elbow = {kl.elbow}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefdcc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k = kl.elbow\n",
    "kmeans = KMeans(n_clusters=5, **kmeans_kwargs)\n",
    "kmeans.fit(X_scaled)\n",
    "rfm['K_Cluster_elbow'] = kmeans.labels_\n",
    "rfm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec823b69",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c9ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "# Assign PCA columns\n",
    "pca_x = X_pca[:, 0]\n",
    "pca_y = X_pca[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(pca_x, pca_y, c=rfm['K_Cluster_elbow'], cmap='viridis', alpha=0.6)\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "centroids_pca = pca.transform(centroids)\n",
    "centroids_x = centroids_pca[:, 0]\n",
    "centroids_y = centroids_pca[:, 1]\n",
    "\n",
    "plt.scatter(centroids_x, centroids_y, marker='D', s=200, c='red')\n",
    "\n",
    "plt.title(f'K-Means Clustering of Customers (k={optimal_k})')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e660bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "centroids_pca = pca.transform(kmeans.cluster_centers_)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],\n",
    "           c=rfm['K_Cluster_elbow'], cmap='viridis', s=50, alpha=0.5)\n",
    "ax.scatter(centroids_pca[:, 0], centroids_pca[:, 1], centroids_pca[:, 2],\n",
    "           c='red', s=400, marker='D', label='Centroids', depthshade=False)\n",
    "ax.set_title('K-Means Clusters (PCA-transformed)')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b04fac3",
   "metadata": {},
   "source": [
    "## Silhouette Method\n",
    "Silhouette method is another method to evaliate clustering quality and help to fin dthe optimal number of clusters(k). \n",
    "\n",
    "It is measureing how similar a data point is to its own cluster compared to other cluster. \n",
    "\n",
    "Silhouette Score Evaluation and range:\n",
    "\n",
    "0.71-1: Strong structure and clusters well-seperated;\n",
    "\n",
    "0.51-0.7: Reasonable structure;\n",
    "\n",
    "0.26-0.5: Weak structure, overlapping clusters\n",
    "\n",
    "< 0.25: No substantial structure\n",
    "\n",
    "Usually higher silhouette Score means the data is well separated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4535ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Method\n",
    "silhouette_coefficients = []\n",
    "\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(X_scaled)\n",
    "    score = silhouette_score(X_scaled, kmeans.labels_)\n",
    "    silhouette_coefficients.append(score)\n",
    "\n",
    "plt.plot(range(2, 11), silhouette_coefficients, marker='o')\n",
    "plt.xticks(range(2, 11))\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Coefficient\")\n",
    "plt.title(\"Silhouette Method\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe7ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = rfm_norm[['Recency_Norm', 'Frequency_Norm', 'Monetary_Norm']].copy()\n",
    "\n",
    "for k in [2,3,4,5,6,7]:\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=k,\n",
    "        init=\"k-means++\",  \n",
    "        n_init=20,        \n",
    "        max_iter=300,\n",
    "        random_state=42  \n",
    "    )\n",
    "    labels = kmeans.fit_predict(rfm_norm)\n",
    "    score = silhouette_score(rfm_norm, labels)\n",
    "    print(f\"K={k}, Silhouette Score={score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90036b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, **kmeans_kwargs)\n",
    "rfm['K_Cluster_silhouette'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "rfm['K_Cluster_silhouette'] = kmeans.labels_\n",
    "rfm.groupby('K_Cluster_silhouette')[['Recency', 'Frequency', 'MonetaryValue']].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace3dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "pca_x = X_pca[:, 0]\n",
    "pca_y = X_pca[:, 1]\n",
    "\n",
    "# Scatter plot of customers, colored by cluster\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(pca_x, pca_y, c=rfm['K_Cluster_silhouette'], cmap='viridis', alpha=0.6)\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "centroids_pca = pca.transform(centroids)\n",
    "centroids_x = centroids_pca[:, 0]\n",
    "centroids_y = centroids_pca[:, 1]\n",
    "\n",
    "plt.scatter(centroids_x, centroids_y, marker='D', s=200, c='red')\n",
    "\n",
    "plt.title(f'K-Means Clustering of Customers (k={optimal_k})')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e8874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "centroids_pca = pca.transform(kmeans.cluster_centers_)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2],\n",
    "           c=rfm['K_Cluster_silhouette'], cmap='viridis', s=50, alpha=0.5)\n",
    "ax.scatter(centroids_pca[:, 0], centroids_pca[:, 1], centroids_pca[:, 2],\n",
    "           c='red', s=400, marker='D', label='Centroids', depthshade=False)\n",
    "ax.set_title('K-Means Clusters (PCA-transformed)')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b318f",
   "metadata": {},
   "source": [
    "## Profilling and Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e6ff91",
   "metadata": {},
   "source": [
    "### Labeling the Cluster Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Data of K_Cluster elbow method\n",
    "K_Cluster_elbow_info = rfm.groupby('K_Cluster_elbow')[['Recency', 'Frequency', 'MonetaryValue']].mean().round(2)\n",
    "K_Cluster_elbow_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbacbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on information above, identify the labels\n",
    "cluster_labels_elbow = {\n",
    "    0: 'New or Occasional Buyers',\n",
    "    1: 'Potential Loyalists',\n",
    "    2: 'Loyal Customers',\n",
    "    3: 'VIP / Champions',\n",
    "    4: 'Lost Customers'\n",
    "}\n",
    "\n",
    "# Create a column for the elbow method labeling\n",
    "rfm['Cluster_Label_elbow'] = rfm['K_Cluster_elbow'].map(cluster_labels_elbow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0273382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check K_Cluster Silhouette method data\n",
    "K_Cluster_silhouette_info = rfm.groupby('K_Cluster_silhouette')[['Recency', 'Frequency', 'MonetaryValue']].mean().round(2)\n",
    "K_Cluster_silhouette_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a76e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on information above, identify the labels\n",
    "cluster_labels_silhouette = {\n",
    "    0: 'Low-value customers',\n",
    "    1: 'High-value customers',\n",
    "}\n",
    "\n",
    "# Create a column for the silhouette method labeling\n",
    "rfm['Cluster_Label_silhouette'] = rfm['K_Cluster_silhouette'].map(cluster_labels_silhouette)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f0e3c1",
   "metadata": {},
   "source": [
    "## Business Insights from K-mean Clustering Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_cluster_summary = rfm.groupby('K_Cluster_elbow')[['Recency', 'Frequency', 'MonetaryValue']].mean()\n",
    "rfm_cluster_summary.rename(index=cluster_labels_elbow, inplace=True)\n",
    "\n",
    "rfm_cluster_norm = (rfm_cluster_summary - rfm_cluster_summary.min()) / (rfm_cluster_summary.max() - rfm_cluster_summary.min())\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(\n",
    "    rfm_cluster_norm,\n",
    "    annot=True,\n",
    "    cmap='YlGnBu',\n",
    "    linewidths=0.5,\n",
    "    fmt=\".2f\"\n",
    ")\n",
    "plt.title('RFM Profile Heatmap by Customer Cluster', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Customer Segments')\n",
    "plt.xlabel('RFM Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f2892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count customers per cluster\n",
    "cluster_counts = rfm['Cluster_Label_elbow'].value_counts().sort_index()\n",
    "\n",
    "# Total revenue per cluster\n",
    "cluster_revenue = rfm.groupby('Cluster_Label_elbow')['MonetaryValue'].sum()\n",
    "\n",
    "# Combine summary\n",
    "summary_elbow = pd.DataFrame({\n",
    "    'CustomerCount': cluster_counts,\n",
    "    'TotalRevenue': cluster_revenue,\n",
    "    'AvgRevenuePerCustomer': cluster_revenue / cluster_counts,\n",
    "    'CustomerPct': (cluster_counts / len(rfm)) * 100,\n",
    "    'RevenuePct': (cluster_revenue / rfm['MonetaryValue'].sum()) * 100\n",
    "}).round(2)\n",
    "\n",
    "print(K_Cluster_elbow_info)\n",
    "\n",
    "summary_elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74172ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Customer % pie chart\n",
    "axes[0].pie(\n",
    "    summary_elbow['CustomerPct'],\n",
    "    labels=summary_elbow.index,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=140,\n",
    "    colors=sns.color_palette(\"pastel\"),\n",
    "    textprops={'fontsize': 10}\n",
    ")\n",
    "axes[0].set_title('Customer Distribution by Segment', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Revenue % pie chart\n",
    "axes[1].pie(\n",
    "    summary_elbow['RevenuePct'],\n",
    "    labels=summary_elbow.index,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=140,\n",
    "    colors=sns.color_palette(\"pastel\"),\n",
    "    textprops={'fontsize': 10}\n",
    ")\n",
    "axes[1].set_title('Revenue Contribution by Segment', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ecbb9",
   "metadata": {},
   "source": [
    "Based on the K-means clustering (5 clusters), we constructed the *K_Cluster_elbow_info* and *summary_elbow* dataframes, in which data was aggregated by cluster labels. The key findings are as follows:\n",
    "\n",
    "**Cluster Mapping**: \n",
    "\n",
    "0: 'New or Occasional Buyers'\n",
    "\n",
    "1: 'Potential Loyalists'\n",
    "\n",
    "2: 'Loyal Customers'\n",
    "\n",
    "3: 'VIP / Champions'\n",
    "\n",
    "4: 'Lost Customers'\n",
    "\n",
    "\n",
    "**Observations**: \n",
    "1. The *VIPs/Champions* make up 7.91% of the customer base; however, they generate 52.93% of the total revenue, driving the most revenue overall. This confirms the Pareto principle, where a small fraction of customers drive most of the revenue. Therefore, retaining or expanding the VIP/Champions group is critical. The recency of this group is 12 days, showing a clear evidence of engagement and ongoing activity. \n",
    "\n",
    "2. Around 45% of the customer base (*Loyal* and *Potential Loyalists*) is moderately engaged, generating nearly 40% of total revenue. This presents a healthy and scalable middle-tier customer segment. These groups represent the best opportunity to increase future revenue.The business would benefit from moving them toward the VIP tier. Providing a more personalized customer experience, including loyalty benefits or exclusive deals, would be a great way to do so.\n",
    "\n",
    "3. For *Potential Loyalists*, the heatmaps indicating 0.48 for recency, showing that they haven't purchased very recently, which their activity level is moderate to declining. Even holding a slight higher frequency and monetary value than *New/Occasional Buyers* but most recent purchase was a while ago. Indicating they are at risk of becoming inactive. \n",
    "-  Since this group represents 22.2% of the customer base and contributes 16.2% of total revenue, they could be the key leverage point for improving overall retention and revenue growth.\n",
    "\n",
    "4. Notice that *New or Occasional Buyers* and *Lost Customers* account for 19.2% and 27.29% of the customer base, respectively. They generate only 3.67% and 3.6% of the total revenue. The average purchase frequencies for these two groups are 1.64 and 1.2.\n",
    "- The *New or Occasional Buyers* (19% of the customer base) show limited engagement.\n",
    "- The *Lost Customers* form the largest cluster (27.29%) but contribute the lowest percentage of revenue (3.6%).\n",
    "- Nearly half of the customers (46%) are inactive, showing weak retention. This presents the biggest churn risk, as the company is experiencing a high rate of attrition.\n",
    "- This percentage suggests that while the company is able to attract customers and drive initial purchases, many customers leave after their first or second transaction.\n",
    "- More attention should be given to product/service quality, potential competitive alternatives, and overall customer satisfaction.\n",
    "\n",
    "**In summary**: \n",
    "\n",
    "The company is currently experiencing a customer base imbalance, where a small number of very loyal, high-spending customers generate the majority of revenue (7% of customers drive over half of total revenue), while a large portion of customers are disengaged (27% are *Lost*, 19% are *New or Occasional Buyers*). Nearly half (46%) of the customer base contributes less than 8% of total revenue, suggesting customer churn and weak retention. And there is exsiting risk where *potential loyalists* are potentially becoming inactive. \n",
    "\n",
    "The company is in a retention-heavy, revenue-concentrated phase, relying on a small group of VIPs for most of its sales while maintaining a large inactive customer base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477e1074",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_cluster_summary = rfm.groupby('K_Cluster_silhouette')[['Recency', 'Frequency', 'MonetaryValue']].mean()\n",
    "rfm_cluster_summary.rename(index=cluster_labels_silhouette, inplace=True)\n",
    "\n",
    "rfm_cluster_norm = (rfm_cluster_summary - rfm_cluster_summary.min()) / (rfm_cluster_summary.max() - rfm_cluster_summary.min())\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(\n",
    "    rfm_cluster_norm,\n",
    "    annot=True,\n",
    "    cmap='YlGnBu',\n",
    "    linewidths=0.5,\n",
    "    fmt=\".2f\"\n",
    ")\n",
    "plt.title('RFM Profile Heatmap by Customer Cluster', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Customer Segments')\n",
    "plt.xlabel('RFM Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ced4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count customers per cluster\n",
    "cluster_counts = rfm['Cluster_Label_silhouette'].value_counts().sort_index()\n",
    "\n",
    "# Total revenue per cluster\n",
    "cluster_revenue = rfm.groupby('Cluster_Label_silhouette')['MonetaryValue'].sum()\n",
    "\n",
    "# Combine summary\n",
    "summary_silhouette = pd.DataFrame({\n",
    "    'CustomerCount': cluster_counts,\n",
    "    'TotalRevenue': cluster_revenue,\n",
    "    'AvgRevenuePerCustomer': cluster_revenue / cluster_counts,\n",
    "    'CustomerPct': (cluster_counts / len(rfm)) * 100,\n",
    "    'RevenuePct': (cluster_revenue / rfm['MonetaryValue'].sum()) * 100\n",
    "}).round(2)\n",
    "\n",
    "print(K_Cluster_silhouette_info)\n",
    "summary_silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb2d001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pie charts for Customer % and Revenue % ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Customer % pie chart\n",
    "axes[0].pie(\n",
    "    summary_silhouette['CustomerPct'],\n",
    "    labels=summary_silhouette.index,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=140,\n",
    "    colors=sns.color_palette(\"pastel\"),\n",
    "    textprops={'fontsize': 10}\n",
    ")\n",
    "axes[0].set_title('Customer Distribution by Segment', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Revenue % pie chart\n",
    "axes[1].pie(\n",
    "    summary_silhouette['RevenuePct'],\n",
    "    labels=summary_silhouette.index,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=140,\n",
    "    colors=sns.color_palette(\"pastel\"),\n",
    "    textprops={'fontsize': 10}\n",
    ")\n",
    "axes[1].set_title('Revenue Contribution by Segment', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1ec6e7",
   "metadata": {},
   "source": [
    "Based on the K-means clustering (5 clusters), we constructed the *K_Cluster_silhouette_info* and *summary_silhouette* dataframe dataframes, in which data was aggregated by cluster labels. The key findings are as follows:\n",
    "\n",
    "**Cluster Mapping**: \n",
    "\n",
    "0: 'Low-value customers'\n",
    "\n",
    "1: 'High-value customers'\n",
    "\n",
    "**Observations**:\n",
    "1. The *low-value customers*'s frequency is 1.67, which highlights that the customer leave after one or second transaction, reensure the low retention rate. \n",
    "\n",
    "2. The *low-value customers* take over 61.58% of the customer base but only showing 14.92% of the total revenue. And h*igh value customers* is hvaing 38.42% customer base and 85.08%. This shown a potential risk as we highly relying on specific customers to product revenue. Presenting a high customer concentration risk. \n",
    "\n",
    "3. *High-value customers* have a much lower recency (≈ 26 days) compared to *low-value customers* (≈ 134 days); and they purchase 8.44 times on average, compared to 1.67 times among *low-value customers*, which is roughly 5x more frequent. And the average spending (4548.26) is nearly 9x higher than *lower-value groups*(497.55). Showing that *high-value customers* are active, loyal, and profitable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c4b1a",
   "metadata": {},
   "source": [
    "## Business Recommendations\n",
    "1. **Retain VIPs and High-Value Customers**\n",
    "- Focus on customers who are active, loyal, and generate the most revenue. Offer loyalty rewards, early access, and personalized offers to further build relationships and trust.\n",
    "- Understand their needs and satisfaction levels to develop more tailored offers. Learn from loyal customers to identify what drives their continued engagement.\n",
    "- Consider implementing advocacy programs that encourage referrals or testimonials.\n",
    "2. **Increase retention rate**\n",
    "- Launch reactivation campaigns based on past customer preferences.\n",
    "- Conduct an in-depth analysis of the customer experience to identify obstacles in the user journey, evaluate competitive alternatives, and assess customer service quality. Determine the key reasons why some customers do not make repeat purchases.\n",
    "- Implement promotional campaigns through personalizaed engagment and early loyalty incentives.\n",
    "- Set up automated campaigns triggered by customer inactivity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9a968d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
